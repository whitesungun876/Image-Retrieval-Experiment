{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea2b25-0bb4-4c3b-ac31-fc3c295fa9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(data_path, categories, img_size=(128, 128), test_size=0.5):\n",
    "    \n",
    "    train_data = {\"images\": [], \"labels\": []}\n",
    "    test_data = {\"images\": [], \"labels\": []}\n",
    "    category_to_id = {category: idx for idx, category in enumerate(categories)}\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(data_path, category)\n",
    "        category_images = []\n",
    "\n",
    "        for filename in os.listdir(category_path):\n",
    "            if filename.endswith('.jpg') or filename.endswith('.jpeg'):\n",
    "                img_path = os.path.join(category_path, filename)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is not None:\n",
    "                        img = cv2.resize(img, img_size) \n",
    "                        category_images.append(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_path}: {e}\")\n",
    "\n",
    "        train_images, test_images = train_test_split(category_images, test_size=test_size, random_state=42)\n",
    "\n",
    "        train_data[\"images\"].extend(train_images)\n",
    "        train_data[\"labels\"].extend([category_to_id[category]] * len(train_images))\n",
    "\n",
    "        test_data[\"images\"].extend(test_images)\n",
    "        test_data[\"labels\"].extend([category_to_id[category]] * len(test_images))\n",
    "\n",
    "    return train_data, test_data, category_to_id\n",
    "\n",
    "\n",
    "def extract_sift_features(images):\n",
    "\n",
    "    sift = cv2.SIFT_create()\n",
    "    all_features = []\n",
    "    for img in images:\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        if descriptors is not None:\n",
    "            all_features.append(descriptors)\n",
    "        else:\n",
    "            all_features.append(np.array([]))  \n",
    "    return all_features\n",
    "\n",
    "\n",
    "def build_visual_vocab(features, vocab_size=100):\n",
    "   \n",
    "    descriptors = np.vstack(features)  \n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=vocab_size, random_state=42, n_init=10, max_iter=300, tol=1e-4)\n",
    "        kmeans.fit(descriptors)  \n",
    "    except Exception as e:\n",
    "        print(f\"Error during K-Means clustering: {e}\")\n",
    "        kmeans = None\n",
    "    return kmeans\n",
    "\n",
    "\n",
    "def compute_bow_histograms(features, kmeans):\n",
    "    \n",
    "    histograms = []\n",
    "    for descriptors in features:\n",
    "        if descriptors.size > 0:\n",
    "            cluster_assignments = kmeans.predict(descriptors)  \n",
    "            hist = np.bincount(cluster_assignments, minlength=kmeans.n_clusters)  \n",
    "        else:\n",
    "            hist = np.zeros(kmeans.n_clusters)  \n",
    "        histograms.append(hist)\n",
    "    return histograms\n",
    "\n",
    "\n",
    "def tf_idf_similarity(hist1, hist2):\n",
    "    \n",
    "    hist1 = hist1 / np.sum(hist1)\n",
    "    hist2 = hist2 / np.sum(hist2)\n",
    "\n",
    "    idf = np.log(len(hist1) / (np.count_nonzero(hist1) + 1)) + 1\n",
    "    \n",
    "    tfidf_hist1 = hist1 * idf\n",
    "    tfidf_hist2 = hist2 * idf\n",
    "    \n",
    "    tfidf_hist1 = normalize([tfidf_hist1])[0]\n",
    "    tfidf_hist2 = normalize([tfidf_hist2])[0]\n",
    "    \n",
    "    return cosine_similarity([tfidf_hist1], [tfidf_hist2])[0][0]\n",
    "\n",
    "\n",
    "def kullback_leibler_divergence(hist1, hist2):\n",
    "\n",
    "    hist1 = hist1 / np.sum(hist1)\n",
    "    hist2 = hist2 / np.sum(hist2)\n",
    "    # Avoid division by zero\n",
    "    hist1 = np.clip(hist1, 1e-10, 1)\n",
    "    hist2 = np.clip(hist2, 1e-10, 1)\n",
    "    return np.sum(hist1 * np.log(hist1 / hist2))\n",
    "\n",
    "\n",
    "def common_words_similarity(hist1, hist2, measure=\"cosine\"):\n",
    "    \n",
    "    if measure == \"cosine\":\n",
    "        return cosine_similarity([hist1], [hist2])[0][0]\n",
    "    elif measure == \"tfidf\":\n",
    "        return tf_idf_similarity(hist1, hist2)\n",
    "    elif measure == \"kl\":\n",
    "        return -kullback_leibler_divergence(hist1, hist2)  \n",
    "\n",
    "\n",
    "def compute_mrr_and_top3(query_bow, train_bows, train_labels, query_label, similarity_measure=\"cosine\"):\n",
    "    \n",
    "    similarities = [common_words_similarity(query_bow, train_bow, similarity_measure) for train_bow in train_bows]\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  \n",
    "    top3_labels = [train_labels[i] for i in sorted_indices[:3]]  \n",
    "\n",
    "    mrr = 0\n",
    "    for rank, label in enumerate(top3_labels, 1):\n",
    "        if label == query_label:\n",
    "            mrr = 1 / rank  # Reciprocal rank\n",
    "            break\n",
    "\n",
    "    top3_accuracy = query_label in top3_labels  \n",
    "    return mrr, top3_accuracy\n",
    "\n",
    "\n",
    "def visualize_retrieval(query_image, top_3_images, top_3_labels, category_to_id):\n",
    "    query_image_rgb = cv2.cvtColor(query_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    axes[0].imshow(query_image_rgb)\n",
    "    axes[0].set_title(\"Query Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    for i in range(3):\n",
    "        axes[i+1].imshow(cv2.cvtColor(top_3_images[i], cv2.COLOR_GRAY2RGB))\n",
    "        axes[i+1].set_title(f\"Top {i+1}: {list(category_to_id.keys())[list(category_to_id.values()).index(top_3_labels[i])]}\")\n",
    "        axes[i+1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def image_retrieval_experiment(train_bows, train_labels, test_bows, test_labels, similarity_measure=\"cosine\"):\n",
    "    \n",
    "    mrrs = []\n",
    "    top3_accuracies = []\n",
    "\n",
    "    for i, test_bow in enumerate(test_bows):\n",
    "        query_label = test_labels[i]\n",
    "        \n",
    "        similarities = [common_words_similarity(test_bow, train_bow, similarity_measure) for train_bow in train_bows]\n",
    "        \n",
    "        sorted_indices = np.argsort(similarities)[::-1]  \n",
    "        top3_labels = [train_labels[i] for i in sorted_indices[:3]]  \n",
    "\n",
    "        mrr = 0\n",
    "        for rank, label in enumerate(top3_labels, 1):\n",
    "            if label == query_label:\n",
    "                mrr = 1 / rank  \n",
    "                break\n",
    "\n",
    "        top3_accuracy = query_label in top3_labels \n",
    "        mrrs.append(mrr)\n",
    "        top3_accuracies.append(top3_accuracy)\n",
    "        \n",
    "        top_3_images = [train_data['images'][i] for i in sorted_indices[:3]]\n",
    "        visualize_retrieval(test_data['images'][i], top_3_images, top3_labels, category_to_id)\n",
    "    \n",
    "    mean_mrr = np.mean(mrrs)\n",
    "    top3_accuracy_percent = np.mean(top3_accuracies) * 100\n",
    "    return mean_mrr, top3_accuracy_percent\n",
    "\n",
    "data_path = '101_ObjectCategories'\n",
    "categories = ['beaver', 'cup', 'brain', 'elephant', 'Faces']\n",
    "\n",
    "train_data, test_data, category_to_id = load_data(data_path, categories)\n",
    "\n",
    "# Extract SIFT features\n",
    "train_features = extract_sift_features(train_data[\"images\"])\n",
    "test_features = extract_sift_features(test_data[\"images\"])\n",
    "\n",
    "# Build visual vocabulary\n",
    "kmeans = build_visual_vocab(train_features, vocab_size=100)\n",
    "\n",
    "# Compute BoW histograms\n",
    "train_bows = compute_bow_histograms(train_features, kmeans)\n",
    "test_bows = compute_bow_histograms(test_features, kmeans)\n",
    "\n",
    "# Experiment 1: Using TF-IDF Similarity\n",
    "mean_mrr_tfidf, top3_accuracy_percent_tfidf = image_retrieval_experiment(\n",
    "    train_bows, train_data[\"labels\"], test_bows, test_data[\"labels\"], similarity_measure=\"tfidf\"\n",
    ")\n",
    "\n",
    "print(f\"\\nExperiment 1 (TF-IDF Similarity):\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mean_mrr_tfidf:.3f}\")\n",
    "print(f\"Top-3 Accuracy: {top3_accuracy_percent_tfidf:.2f}%\")\n",
    "\n",
    "# Experiment 2: Using KL Divergence\n",
    "mean_mrr_kl, top3_accuracy_percent_kl = image_retrieval_experiment(\n",
    "    train_bows, train_data[\"labels\"], test_bows, test_data[\"labels\"], similarity_measure=\"kl\"\n",
    ")\n",
    "\n",
    "print(f\"\\nExperiment 2 (KL Divergence):\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mean_mrr_kl:.3f}\")\n",
    "print(f\"Top-3 Accuracy: {top3_accuracy_percent_kl:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043b240-3f19-4004-9f12-8c5270a62e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
